# Clustering and Classification chapter 

*In this section, we will describe the steps taken to perform clustering and classification analysis of Boston data*

### Task 2
```{r}
# In this begining part we shall be loading the Boston data from MASS

# Lest us access the MASS library 
library(MASS)

#Since the library was successfully accessed. We now load the Boston data
data("Boston")

# Let us explore the dataset and see its structure, dimension, and summary 
str(Boston)     # structure of Boston data
dim(Boston)     # dimention of Boston data
summary(Boston)
```

Boston is a data frame with 14 variables and 506 observations. The variables descriptions are provided in this link https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html 

### Task 3: Showing graphical overview of the data

```{r}
# We wouls like to show the graphical overview of the data by calling the pairs function 
pairs(Boston)

```
Figure 1 shows the matrix graph of Boston data


```{r}
# Here we summaries of variables (columns) of the Boston data
colnames(Boston)

```

The detailed explanation of each variable is presented in this list below

1. *crim* - per capita crime rate by town.
2. *zn* - proportion of residential land zoned for lots over 25,000 sq.ft.
3. *indus* - proportion of non-retail business acres per town.
4. *chas* - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
5. *nox* - nitrogen oxides concentration (parts per 10 million).
6. *rm* - average number of rooms per dwelling.
7. *age* - proportion of owner-occupied units built prior to 1940.
8. *dis* - weighted mean of distances to five Boston employment centres.
9. *rad* - index of accessibility to radial highways.
10. *tax* - full-value property-tax rate per \$10,000.
11. *ptratio* - pupil-teacher ratio by town.
12. *black* - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
13. *lstat* - lower status of the population (percent).
14. *medv* - median value of owner-occupied homes in \$1000s.


### Task4: Standardizing the dataset and print out summaries
```{r}
# here we are standardizing the data by scaling using the scale() function 
boston_scaled <- scale(Boston)

# We shall print the summary of the scaled data by using the function symmary()
summary(boston_scaled)
```

The variables of the "scaled data" all have their mean to be zero (0)

```{r}
# Lets us check whether the scaled data is data frame or not by using is.data.frame() function
is.data.frame(boston_scaled)

```

Note that the scaled data is not data frame. We will now convert the boston_scale to data frame by usinf the next step.
```{r}
# Making the boston_scaled data a data frame 
boston_scaled <- as.data.frame(boston_scaled)
is.data.frame(boston_scaled)

```

#### Task 4.2 Create a categorical variable of the crime rate in the Boston dataset
Now the boston_scaled is now a data frame. Hence, we can continue other analysis.

```{r}
#Let us examine the crime variable of the scaled data
summary(boston_scaled$crim)


# create a quantile vector of crime 
q_vector <- quantile(boston_scaled$crim)
# Let us print the quantile vector of crime rate
q_vector

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = q_vector, include.lowest = TRUE, label= c("low", "med_low", "med_high", "high"))
# Let us now use the table() function to see the new factor crime
table(crime)
```
The crime rate in Boston data now shows on the levels low to high.

#### Task 4.3 Droping the the old crime

```{r}
# Lests remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# here are would add the new crime to boston_scaled 
boston_scaled <- data.frame(boston_scaled, crime)

```
#### Task 4.4 Divide the dataset to train and test sets

```{r}
 # Let us asign the number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
```

Now we will divide divide the dataset to train and test sets
```{r}
# Lets create train set with 80%
train <- boston_scaled[ind,]

# Lets create test set  
test <- boston_scaled[-ind,]
```

### Task 5. Fit the linear discriminant analysis on the train set

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# Lets us print the lda.fit object
lda.fit
```
Based on the linear discriminant analysis LDA of the train dataset,where the crime rate is the target variable and other variables are the predictors, we can see that the crime rate is 26% low , 26% med_low, 23% med_high, and 25% high.

##### Now let us draw the LDA (bi)plot


```{r}
# 
# Here is the function for lda biplot arrows copied from Datacamp
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}


# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

## Interpreting this graph is a bit difficult for me as a non statistician :) 

### Task 6
```{r}

# save the correct classes from test data
correct_classes <- test$crime

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

Observing the predicted values in the table, the diagonal values (13,13,31,27) indicates ....

### Task 7. 

